{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Various feature generation methods\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas\n",
    "import spacy\n",
    "import re\n",
    "import math\n",
    "from geotext import GeoText\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "question_tokens = set([\"why\", \"how\", \"what\", \"when\", \"which\", \"who\", \"whose\", \"whom\"])\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1.0 / (count + eps)\n",
    "\n",
    "\n",
    "def remove_punc(s):\n",
    "    return re.sub(r'[^\\w\\s]', '', UnicodeDammit(str(s)).markup)\n",
    "\n",
    "\n",
    "def clean_statement(s):\n",
    "    \"\"\"\n",
    "    Remove punctuation, stop words and standardise casing\n",
    "    words, and return remaining tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove punctuation\n",
    "    s = remove_punc(s)\n",
    "    sentence = nlp(s)\n",
    "    sentence_with_stop_checks = [(sentence[i], sentence[i].is_stop) for i in range(len(sentence))]\n",
    "\n",
    "    return sorted([w for (w, stop_bool) in sentence_with_stop_checks if not stop_bool])\n",
    "\n",
    "\n",
    "def token_overlap_score(row):\n",
    "    \"\"\"\n",
    "    appending column of set overlap percentage, where each set\n",
    "    is a set of tokens excluding stop-words and punctuation,\n",
    "    and in lemmatised form\n",
    "    \"\"\"\n",
    "    cleaned_question1_words = clean_statement(row[\"question1\"])\n",
    "    cleaned_question2_words = clean_statement(row[\"question2\"])\n",
    "    \n",
    "    set1, set2 = \\\n",
    "            (set([w.lemma_.lower() for w in cleaned_question1_words]),\n",
    "             set([w.lemma_.lower() for w in cleaned_question2_words]))\n",
    "        \n",
    "    return 0.0 if not len(set1.union(set2)) else 1.0 * len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "\n",
    "\n",
    "def weighted_token_overlap_score(row):\n",
    "    cleaned_question1_words = clean_statement(row[\"question1\"])\n",
    "    cleaned_question2_words = clean_statement(row[\"question2\"])\n",
    "    \n",
    "    set1, set2 = \\\n",
    "            (set([w.lemma_.lower() for w in cleaned_question1_words]),\n",
    "             set([w.lemma_.lower() for w in cleaned_question2_words]))\n",
    "        \n",
    "    return (1.0 * len(set1.intersection(set2)) / (len(set1.union(set2)) or 1)) * \\\n",
    "            (\n",
    "                min(len(str(row[\"question1\"])), len(str(row[\"question2\"]))) / \n",
    "                (1.0 * max(len(str(row[\"question1\"])), len(str(row[\"question2\"]))))\n",
    "            )\n",
    "\n",
    "\n",
    "def noun_phrase_overlap(row):\n",
    "    q1_doc = nlp(UnicodeDammit(str(row[\"question1\"])).markup)\n",
    "    q2_doc = nlp(UnicodeDammit(str(row[\"question2\"])).markup)\n",
    "    q1_np = set([noun_p.text for noun_p in q1_doc.noun_chunks])\n",
    "    q2_np = set([noun_p.text for noun_p in q2_doc.noun_chunks])\n",
    "    return len(q1_np.intersection(q2_np)) / (float(len(q1_np.union(q2_np))) or 1.0)\n",
    "\n",
    "\n",
    "def question_length_ratio(row):\n",
    "    return min(float(len(str(row[\"question1\"]))) / len(str(row[\"question2\"])), 5)\n",
    "\n",
    "\n",
    "def punctuation_sym_ratio(row):\n",
    "    return min(\n",
    "        5,\n",
    "        (1.0 * len(re.split(r'[^\\w\\s]', UnicodeDammit(str(row[\"question1\"])).markup))) / \n",
    "        len(re.split(r'[^\\w\\s]', UnicodeDammit(str(row[\"question2\"])).markup))\n",
    "    )\n",
    "\n",
    "\n",
    "def countries_mentioned_overlap(row):\n",
    "    q1 = remove_punc(row[\"question1\"])\n",
    "    q2 = remove_punc(row[\"question2\"])\n",
    "    q1_geo = GeoText(\". \".join([w.upper() for w in q1.split(\" \")]))\n",
    "    q1_countries = set([k for (k, v) in q1_geo.country_mentions.items()])\n",
    "    q2_geo = GeoText(\". \".join([w.upper() for w in q2.split(\" \")]))\n",
    "    q2_countries = set([k for (k, v) in q2_geo.country_mentions.items()])\n",
    "    \n",
    "    return float(len(q1_countries.intersection(q2_countries))) / (len(q1_countries.union(q2_countries)) or 1.0)\n",
    "\n",
    "\n",
    "def stops_ratios(row):\n",
    "    q1_tokens = [t.lower() for t in remove_punc(row[\"question1\"]).split()]\n",
    "    q2_tokens = [t.lower() for t in remove_punc(row[\"question2\"]).split()]\n",
    "    q1_stops = set([t for t in q1_tokens if t in stops])\n",
    "    q2_stops = set([t for t in q2_tokens if t in stops])\n",
    "    return (\n",
    "        float(len(q1_stops.intersection(q2_stops))) / (len(q1_stops.union(q2_stops)) or 1.0),\n",
    "        float(len(q1_stops)) / (len(q1_tokens) or 1.0),\n",
    "        float(len(q2_stops)) / (len(q2_tokens) or 1.0),\n",
    "        math.fabs(float(len(q1_stops)) / (len(q1_tokens) or 1.0) - float(len(q2_stops)) / (len(q2_tokens) or 1.0))\n",
    "    )\n",
    "\n",
    "\n",
    "def question_tokens_ratio(row):\n",
    "    q1_quest_tokens = set([t.lower() for t in remove_punc(row[\"question1\"]).split() if t.lower() in question_tokens])\n",
    "    q2_quest_tokens = set([t.lower() for t in remove_punc(row[\"question2\"]).split() if t.lower() in question_tokens])\n",
    "    return (\n",
    "        float(len(q1_quest_tokens.intersection(q2_quest_tokens))) / (len(q1_quest_tokens.union(q2_quest_tokens)) or 1.0)\n",
    "    )\n",
    "\n",
    "\n",
    "def num_sentences_ratio(row, thres_mult=2.0):\n",
    "    return float(\n",
    "        float(len(str(row[\"question1\"]).split(\".\"))) / len(str(row[\"question2\"]).split(\".\")) >= thres_mult\n",
    "        or\n",
    "        float(len(str(row[\"question1\"]).split(\".\"))) / len(str(row[\"question2\"]).split(\".\")) <  1 / thres_mult\n",
    "    )\n",
    "\n",
    "\n",
    "def punc_blocks_ratio(row):\n",
    "    return min(\n",
    "        10,\n",
    "        math.fabs(len(re.split(r\"[,-.]+\", str(row[\"question1\"]))) - len(re.split(r\"[,-.]+\", str(row[\"question2\"]))))\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_var_dist_by_label(df, var_name, var_calc_fun, bins=20):\n",
    "    \"\"\"\n",
    "    get a DF along with a callback to compute a specific feature\n",
    "    and plot a distribution of the variable split by label\n",
    "    \"\"\"\n",
    "    df[var_name] = df.apply(var_calc_fun, axis=1)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.hist(df[var_name][df['is_duplicate'] == 0], bins=bins, normed=True, label='Not Duplicate')\n",
    "    plt.hist(df[var_name][df['is_duplicate'] == 1], bins=bins, normed=True, alpha=0.7, label='Duplicate')\n",
    "    plt.legend()\n",
    "    plt.title('Label distribution over %s' % var_name, fontsize=15)\n",
    "    plt.xlabel(var_name, fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "train_path = \"/Users/mohamedabdelbary/Documents/kaggle_quora/train.csv\"\n",
    "models_path = \"/Users/mohamedabdelbary/Documents/kaggle_quora/models.pkl\"\n",
    "train_pred_path = \"/Users/mohamedabdelbary/Documents/kaggle_quora/train_preds.csv\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "\n",
    "def read_data(path):\n",
    "    return pandas.read_csv(path)\n",
    "\n",
    "df = read_data(train_path)\n",
    "\n",
    "questions = pandas.Series(df['question1'].tolist() + df['question2'].tolist()).astype(str)\n",
    "questions = [remove_punc(q).lower() for q in questions]\n",
    "eps = 500 \n",
    "words = (\" \".join(questions)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count, eps=eps) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To exit: use 'exit', 'quit', or Ctrl-D.\n"
     ]
    }
   ],
   "source": [
    "# show_var_dist_by_label(df, 'token_overlap_score', token_overlap_score, bins=20)\n",
    "# show_var_dist_by_label(df, 'question_length_ratio', question_length_ratio, bins=20)\n",
    "# show_var_dist_by_label(df, 'punctuation_sym_ratio', punctuation_sym_ratio, bins=20)\n",
    "# show_var_dist_by_label(df, 'noun_phrase_overlap', noun_phrase_overlap, bins=20) # Good feature\n",
    "# show_var_dist_by_label(df, 'weighted_token_overlap_score', weighted_token_overlap_score, bins=20) # Very good feature\n",
    "# show_var_dist_by_label(df, 'countries_mentioned_overlap', countries_mentioned_overlap, bins=20) # Good if you have countries in q\n",
    "# show_var_dist_by_label(df, 'question_tokens_ratio', question_tokens_ratio, bins=20) # good\n",
    "# show_var_dist_by_label(df, 'num_sentences_ratio', num_sentences_ratio, bins=20) # not very useful\n",
    "show_var_dist_by_label(df, 'punc_blocks_ratio', punc_blocks_ratio, bins=20) # good feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pattern.en import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Part-of-speech tags\n",
    "# General\n",
    "# CC\n",
    "# DT\n",
    "# EX\n",
    "# IN\n",
    "\n",
    "# Adjective\n",
    "# JJ\n",
    "# JJR\n",
    "# JJS\n",
    "\n",
    "# Noun\n",
    "# NN\n",
    "# NNS\n",
    "# NNP\n",
    "# NNPS\n",
    "\n",
    "# Pronoun\n",
    "# PRP\n",
    "\n",
    "# Adverb\n",
    "# RB\n",
    "# RBR\n",
    "# RBS\n",
    "\n",
    "# Verb\n",
    "# VB\n",
    "# VBZ\n",
    "# VBP\n",
    "# VBN\n",
    "# VBG\n",
    "\n",
    "# Chunk tags\n",
    "# NP\n",
    "\n",
    "# PP\n",
    "\n",
    "# VP\n",
    "\n",
    "# ADVP\n",
    "\n",
    "# ADJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = parse('What are the best product manager programs that someone in early 20s can join to learn product management?', relations=True).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'What', u'WP', u'O', u'O', u'O'],\n",
       " [u'are', u'VBP', u'B-VP', u'O', u'VP-1'],\n",
       " [u'the', u'DT', u'B-NP', u'O', u'NP-OBJ-1'],\n",
       " [u'best', u'JJS', u'I-NP', u'O', u'NP-OBJ-1'],\n",
       " [u'associate', u'JJ', u'I-NP', u'O', u'NP-OBJ-1'],\n",
       " [u'product', u'NN', u'I-NP', u'O', u'NP-OBJ-1'],\n",
       " [u'manager', u'NN', u'I-NP', u'O', u'NP-OBJ-1'],\n",
       " [u'(', u'(', u'O', u'O', u'O'],\n",
       " [u'APM', u'NNP', u'B-NP', u'O', u'O'],\n",
       " [u')', u')', u'O', u'O', u'O'],\n",
       " [u'programs', u'NNS', u'B-NP', u'O', u'O'],\n",
       " [u'that', u'IN', u'B-PP', u'B-PNP', u'O'],\n",
       " [u'someone', u'NN', u'B-NP', u'I-PNP', u'O'],\n",
       " [u'in', u'IN', u'B-PP', u'B-PNP', u'O'],\n",
       " [u'their', u'PRP$', u'B-NP', u'I-PNP', u'O'],\n",
       " [u'early', u'JJ', u'B-ADJP', u'O', u'O'],\n",
       " [u'20s', u'CD', u'O', u'O', u'O'],\n",
       " [u'can', u'MD', u'B-VP', u'O', u'VP-2'],\n",
       " [u'join', u'VB', u'I-VP', u'O', u'VP-2'],\n",
       " [u'to', u'TO', u'I-VP', u'O', u'VP-2'],\n",
       " [u'learn', u'VB', u'I-VP', u'O', u'VP-2'],\n",
       " [u'product', u'NN', u'B-NP', u'O', u'NP-OBJ-2'],\n",
       " [u'management', u'NN', u'I-NP', u'O', u'NP-OBJ-2'],\n",
       " [u'and', u'CC', u'O', u'O', u'O'],\n",
       " [u'have', u'VBP', u'B-VP', u'O', u'O'],\n",
       " [u'a', u'DT', u'O', u'O', u'O'],\n",
       " [u'rewarding', u'VBG', u'B-VP', u'O', u'VP-3'],\n",
       " [u'career', u'NN', u'B-NP', u'O', u'NP-OBJ-3'],\n",
       " [u'in', u'IN', u'B-PP', u'B-PNP', u'O'],\n",
       " [u'the', u'DT', u'B-NP', u'I-PNP', u'O'],\n",
       " [u'company', u'NN', u'I-NP', u'I-PNP', u'O'],\n",
       " [u'?', u'.', u'O', u'O', u'O']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the', u'that', u'in', u'and', u'a', u'in', u'the']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns = [n[0] for n in x[0] if n[1] in ['NN', 'NNS', 'NNP', 'NNPS']]\n",
    "verbs = [v[0] for v in x[0] if v[1] in ['VB', 'VBZ', 'VBP', 'VBN', 'VBG']]\n",
    "adj = [v[0] for v in x[0] if v[1] in ['JJ', 'JJR', 'JJS']]\n",
    "adv = [v[0] for v in x[0] if v[1] in ['RB', 'RBR', 'RBS']]\n",
    "gen = [v[0] for v in x[0] if v[1] in ['CC', 'DT', 'EX', 'IN']]\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'f', u'NN', u'B-NP', u'O', u'O']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse('f', relations=True).split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
